{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TTTT\\nlp\\.conda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_skills' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m\n",
      "\u001b[0;32m     22\u001b[0m salary_output \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalary_output\u001b[39m\u001b[38;5;124m'\u001b[39m)(common)\n",
      "\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Skill prediction branch\u001b[39;00m\n",
      "\u001b[1;32m---> 25\u001b[0m skill_output \u001b[38;5;241m=\u001b[39m Dense(\u001b[43mnum_skills\u001b[49m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskill_output\u001b[39m\u001b[38;5;124m'\u001b[39m)(common)\n",
      "\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Model compilation\u001b[39;00m\n",
      "\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_text, outputs\u001b[38;5;241m=\u001b[39m[salary_output, skill_output])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_skills' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Input layer\n",
    "input_text = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "num_skills = 256\n",
    "embedding_dim=40\n",
    "vocab_size = len(Word2Vec_model.wv)\n",
    "\n",
    "# Embedding layer for text input\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "\n",
    "x = LSTM(64)(x)\n",
    "\n",
    "# Common features layer\n",
    "common = Dense(128, activation='relu')(x)\n",
    "common = Dropout(0.5)(common)\n",
    "\n",
    "# Salary prediction branch\n",
    "salary_output = Dense(1, activation='linear', name='salary_output')(common)\n",
    "\n",
    "# Skill prediction branch\n",
    "skill_output = Dense(num_skills, activation='sigmoid', name='skill_output')(common)\n",
    "\n",
    "# Model compilation\n",
    "model = Model(inputs=input_text, outputs=[salary_output, skill_output])\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss={'salary_output': 'mse', 'skill_output': 'binary_crossentropy'},\n",
    "              metrics={'salary_output': 'mae', 'skill_output': 'accuracy'})\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D, Embedding, Dropout\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define input layer\n",
    "input_text = Input(shape=(None,), dtype='float32')  # None allows for variable sequence lengths\n",
    "\n",
    "Word2Vec_model = Word2Vec.load(r\"C:\\TTTT\\nlp\\models\\Word2Vec.model\")\n",
    "num_skills = 256\n",
    "embedding_dim=128\n",
    "vocab_size = len(Word2Vec_model.wv)\n",
    "\n",
    "\n",
    "# Embedding and Encoder\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "# Shared layers\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Global average pooling to reduce sequence to a single vector\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Salary output\n",
    "salary_output = Dense(1, activation='linear', name='salary_output')(x)\n",
    "\n",
    "# Skill output (transformer decoder setup could be added here for dynamic sequence generation)\n",
    "# Placeholder for Transformer decoder - in practice, this will be more complex and involve looped decoding\n",
    "skill_output = Dense(num_skills, activation='sigmoid', name='skill_output')(x)  # This is an illustrative placeholder\n",
    "\n",
    "# Complete the model\n",
    "model = Model(inputs=input_text, outputs=[salary_output, skill_output])\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'salary_output': 'mse', 'skill_output': 'binary_crossentropy'},\n",
    "              metrics={'salary_output': 'mae', 'skill_output': 'accuracy'})\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
